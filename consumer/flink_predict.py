import os
import json
import requests
import logging
from pyflink.common import Types, Time, WatermarkStrategy, Configuration
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
from pyflink.datastream.functions import ProcessWindowFunction
from pyflink.datastream.window import TumblingProcessingTimeWindows
from pyflink.common.serialization import SimpleStringSchema

# --- C·∫§U H√åNH ---
KAFKA_BROKERS = "localhost:9092"
TOPIC_NAME = "iot-traffic-congestion"
# Thay b·∫±ng IP th·∫≠t m√°y b·∫°n (ƒë√£ t√¨m ƒë∆∞·ª£c ·ªü b∆∞·ªõc tr∆∞·ªõc)
API_URL = "http://127.0.0.1:8127/predict" 
JAR_FILE_NAME = "flink-sql-connector-kafka-3.0.0-1.17.jar"

class PredictionWindowProcess(ProcessWindowFunction):
    """
    Logic n√¢ng cao:
    1. T·ª± ƒë·ªông Padding (ƒëi·ªÅn khuy·∫øt) n·∫øu thi·∫øu d·ªØ li·ªáu.
    2. T·ª± ƒë·ªông c·∫Øt b·ªõt n·∫øu th·ª´a d·ªØ li·ªáu.
    3. Yield k·∫øt qu·∫£ thay v√¨ Print ƒë·ªÉ tr√°nh l·ªói NoneType.
    """
    def process(self, key, context, elements):
        sensor_id = key
        
        # 1. S·∫Øp x·∫øp theo th·ªùi gian
        sorted_elements = sorted(elements, key=lambda x: x['timestamp'])
        current_len = len(sorted_elements)
        
        target_records = []

        # --- LOGIC CH·ªêNG M·∫§T D·ªÆ LI·ªÜU ---
        
        # Tr∆∞·ªùng h·ª£p 1: Qu√° √≠t d·ªØ li·ªáu (v√≠ d·ª• sensor m·ªõi kh·ªüi ƒë·ªông) -> B·ªè qua
        if current_len < 3:
            yield f"‚ö†Ô∏è [SKIP] {sensor_id}: D·ªØ li·ªáu qu√° √≠t ({current_len}/5) -> Kh√¥ng th·ªÉ Padding."
            return

        # Tr∆∞·ªùng h·ª£p 2: Thi·∫øu 1-2 b·∫£n tin (M·∫°ng lag) -> PADDING (Nh√¢n b·∫£n d√≤ng cu·ªëi)
        elif current_len < 5:
            missing = 5 - current_len
            last_item = sorted_elements[-1]
            
            # Copy d√≤ng cu·ªëi c√πng l·∫•p v√†o ch·ªó tr·ªëng
            padded_elements = sorted_elements + [last_item] * missing
            target_records = padded_elements
            # yield f"‚ÑπÔ∏è [INFO] {sensor_id}: ƒê√£ fix d·ªØ li·ªáu (G·ªëc: {current_len} -> Padding: 5)"

        # Tr∆∞·ªùng h·ª£p 3: ƒê·ªß ho·∫∑c Th·ª´a (Do m·ªü r·ªông Window) -> L·∫•y 5 d√≤ng m·ªõi nh·∫•t
        else:
            target_records = sorted_elements[-5:]

        # -------------------------------

        # 2. Tr√≠ch xu·∫•t Feature (Input cho Model)
        data_batch = []
        for row in target_records:
            feature_row = [
                row['traffic_volume'],
                row['avg_vehicle_speed'],
                row['vehicle_breakdown']['cars'],
                row['vehicle_breakdown']['trucks'],
                row['vehicle_breakdown']['bikes']
            ]
            data_batch.append(feature_row)

        # 3. G·ªçi API (C√≥ Timeout ƒë·ªÉ kh√¥ng treo Flink)
        payload = {"data": data_batch}
        
        try:
            headers = {'Content-Type': 'application/json', 'accept': 'application/json'}
            # Timeout c·ª±c quan tr·ªçng: N·∫øu API treo qu√° 1s, Flink s·∫Ω b·ªè qua ƒë·ªÉ x·ª≠ l√Ω c√°i kh√°c
            response = requests.post(API_URL, json=payload, headers=headers, timeout=1)
            
            if response.status_code == 200:
                result = response.json()
                pred = result.get('prediction', 'Unknown')
                
                # Mapping k·∫øt qu·∫£ cho d·ªÖ hi·ªÉu
                status_map = {0: "üü¢ Th√¥ng tho√°ng", 1: "üü° B√¨nh th∆∞·ªùng", 2: "üî¥ T·∫ÆC NGH·∫ºN"}
                status_text = status_map.get(pred, f"Label {pred}")

                yield (
                    f"üîÆ {sensor_id} | "
                    f"Win: {current_len} recs | "
                    f"InputVol: {data_batch[-1][0]} | "
                    f"üëâ {status_text}"
                )
            else:
                yield f"‚ùå API Error {response.status_code}"
                
        except Exception as e:
            yield f"‚ùå L·ªói k·∫øt n·ªëi API: {str(e)}"

def main():
    # C·∫•u h√¨nh Web UI
    config = Configuration()
    config.set_string("rest.port", "8081")
    config.set_string("rest.address", "localhost")
    env = StreamExecutionEnvironment.get_execution_environment(config)
    
    # Load JAR
    current_dir = os.getcwd()
    jar_path = f"file://{os.path.join(current_dir, JAR_FILE_NAME)}"
    print(f"üîÑ Loading JAR: {jar_path}")
    try:
        env.add_jars(jar_path)
    except:
        return

    print("üöÄ Flink Job Started... (Web UI: http://localhost:8081)")

    source = KafkaSource.builder() \
        .set_bootstrap_servers(KAFKA_BROKERS) \
        .set_topics(TOPIC_NAME) \
        .set_group_id("flink-traffic-group-optimized") \
        .set_starting_offsets(KafkaOffsetsInitializer.latest()) \
        .set_value_only_deserializer(SimpleStringSchema()) \
        .build()

    ds = env.from_source(source, WatermarkStrategy.no_watermarks(), "Kafka Source")

    ds \
        .map(lambda x: json.loads(x), output_type=Types.PICKLED_BYTE_ARRAY()) \
        .key_by(lambda x: x['sensor_id']) \
        .window(TumblingProcessingTimeWindows.of(Time.seconds(28))) \
        .process(PredictionWindowProcess()) \
        .print()
    env.execute("Traffic Prediction Job (Robust)")

if __name__ == '__main__':
    logging.basicConfig(level=logging.ERROR)
    main()